{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Merchants sometimes run big promotions (e.g., discounts or cash coupons) on particular dates (e.g., Boxing-day Sales, \"Black Friday\" or \"Double 11 (Nov 11th)\" , in order to attract a large number of new buyers. Unfortunately, many of the attracted buyers are one-time deal hunters, and these promotions may have little long lasting impact on sales. To alleviate this problem, it is important for merchants to identify who can be converted into repeated buyers. By targeting on these potential loyal customers, merchants can greatly reduce the promotion cost and enhance the return on investment (ROI). It is well known that in the field of online advertising, customer targeting is extremely challenging, especially for fresh buyers. However, with the long-term user behavior log accumulated by Tmall.com, we may be able to solve this problem. In this challenge, we provide a set of merchants and their corresponding new buyers acquired during the promotion on the \"Double 11\" day. Your task is to predict which new buyers for given merchants will become loyal customers in the future. In other words, you need to predict the probability that these new buyers would purchase items from the same merchants again within 6 months. a data set containing around 200k users is given for training, while the other of similar size for testing. Similar to other competitions, you may extract any features, then perform training with additional tools. You need to only submit the prediction results for evaluation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "mms = MinMaxScaler()\n",
    "# from mlxtend.classifier import StackingClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import KFold \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import catboost as cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part1: Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_info=pd.read_csv('taobao/data_format1/user_info_format1.csv')\n",
    "user_info=reduce_mem_usage(user_info)\n",
    "user_log=pd.read_csv('taobao/data_format1/user_log_format1.csv')\n",
    "user_log =reduce_mem_usage(user_log)\n",
    "user_log.rename(columns={'seller_id': 'merchant_id'}, inplace=True)\n",
    "train= pd.read_csv('taobao/data_format1/train_format1.csv')\n",
    "train=reduce_mem_usage(train)\n",
    "# train.rename(columns={'merchant_id': 'seller_id'}, inplace=True)  \n",
    "test= pd.read_csv('taobao/data_format1/test_format1.csv')\n",
    "test=reduce_mem_usage(test)\n",
    "# test.rename(columns={'merchant_id': 'seller_id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill na\n",
    "missingIndex = user_log[user_log.brand_id.isnull()].index\n",
    "sellerMode = user_log.groupby(['merchant_id']).apply(lambda x: x.brand_id.mode()[0]).reset_index()\n",
    "pickUP = user_log.loc[missingIndex]\n",
    "pickUP = pd.merge(pickUP, sellerMode, how='left', on=['merchant_id'])[0].astype('float32')\n",
    "pickUP.index = missingIndex\n",
    "user_log.loc[missingIndex, 'brand_id'] = pickUP\n",
    "del pickUP, sellerMode, missingIndex\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_info.age_range.fillna(user_info.age_range.median(),inplace=True)\n",
    "user_info.gender.fillna(user_info.gender.mode()[0],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part2: Get Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "importt pandas as pd\n",
    "from scipy import sparse\n",
    "from collections import Counter\n",
    "import time\n",
    "import gc\n",
    "\n",
    "class dataFeature(object):\n",
    "\n",
    "    def __init__(self,u_log,train,test):\n",
    "        # self.u_info = u_info\n",
    "        self.u_log = u_log\n",
    "        self.train = train\n",
    "        self.test = test\n",
    "    \n",
    "    def summary(self,key,gbname,pname,prefix,operator,dummy,ifratio):\n",
    "        # user_info = self.u_info\n",
    "        user_log = self.u_log\n",
    "        train = self.train\n",
    "        test = self.test\n",
    "        if operator==None:\n",
    "            if not dummy:\n",
    "                if prefix == None:# count\n",
    "                    df = user_log.groupby(key).size().reset_index().rename(columns={0:pname})\n",
    "                else: # unique\n",
    "                    df = user_log.groupby(key).agg({gbname: lambda x: len(set(x))}).reset_index().rename(columns={gbname: pname})\n",
    "            else:\n",
    "                df = user_log.groupby(key +[gbname]).size().reset_index().rename(columns= {0: pname})\n",
    "                df = pd.get_dummies(df, columns=[gbname], prefix=prefix)\n",
    "                df = df.apply(pd.to_numeric, downcast='unsigned')\n",
    "                columns = [i for i in df.columns.tolist() if prefix in i]\n",
    "                for col in columns:\n",
    "                    df[col] *= df[pname]\n",
    "                df = df.groupby(key).sum().reset_index().drop([pname], axis=1)\n",
    "\n",
    "        else:\n",
    "            if not dummy:\n",
    "                if prefix != None: #mean max min\n",
    "                    df = user_log.groupby(key + [gbname]).size().reset_index().rename(columns={0: pname})\n",
    "                    df = df.groupby(key).agg({pname: operator}).reset_index()\n",
    "                    df.columns = key + [ prefix + 'count', prefix + 'mean', prefix + 'max', prefix + 'min']\n",
    "                else:\n",
    "                    df = user_log.groupby(key).agg({gbname: operator}).reset_index()\n",
    "                    df.rename(columns={gbname: pname}, inplace=True)\n",
    "            else:\n",
    "                if not ifratio:#get_dummies\n",
    "                    df = user_log.groupby([key[0]] + [gbname]).agg({key[1]: operator}).reset_index().rename(columns=\n",
    "                                                                                      {key[1]: pname})\n",
    "                    df = pd.get_dummies(df, columns=[gbname], prefix=prefix)\n",
    "                    df = df.apply(pd.to_numeric, downcast='unsigned')\n",
    "                    columns = [i for i in df.columns.tolist() if prefix in i]\n",
    "                    for col in columns:\n",
    "                        df[col] *= df[pname]\n",
    "                    df = df.groupby(key[0]).sum().reset_index().drop([pname], axis=1)\n",
    "                else:   #ratio \n",
    "                    df = user_log.groupby(key).agg({gbname:lambda x:len(set(x))}).reset_index().rename(columns={gbname: 'all_cnt'})\n",
    "                    df = df.merge(user_log.groupby(key).agg({gbname: operator}).reset_index(), on=key, how='left',copy=False)\n",
    "                    df[pname] = df[gbname] / (df['all_cnt'] + 10)\n",
    "                    df[pname] = df[pname].astype('float32')\n",
    "                    columns = df[key[1]].unique().tolist()\n",
    "                    print(df.dtypes)\n",
    "                    df = pd.get_dummies(df, columns=[key[1]], prefix=prefix)\n",
    "                    for col in columns:\n",
    "                        df[prefix +'_'+ str(col)] *= df[pname]\n",
    "                    df = df.groupby(key[0]).sum().reset_index().drop(['all_cnt', gbname, pname], 1)\n",
    "\n",
    "        if  operator==None:\n",
    "            train = train.merge(df, on=key, how='left', copy=False)\n",
    "            test = test.merge(df, on=key, how='left', copy=False)\n",
    "            self.train = train\n",
    "            self.test = test\n",
    "        else:\n",
    "            if not dummy:\n",
    "                train = train.merge(df, on=key, how='left', copy=False)\n",
    "                test = test.merge(df, on=key, how='left', copy=False)\n",
    "                self.train = train\n",
    "                self.test = test\n",
    "            else:\n",
    "                if not ifratio:\n",
    "                    train = train.merge(df, on=key[0], how='left', copy=False)\n",
    "                    test = test.merge(df, on=key[0], how='left', copy=False)\n",
    "                    self.train = train\n",
    "                    self.test = test\n",
    "                else:\n",
    "                    train = train.merge(df, on=key, how='left', copy=False)\n",
    "                    test = test.merge(df, on=key, how='left', copy=False)\n",
    "                    self.train = train\n",
    "                    self.test = test\n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    feature = dataFeature(user_log,train,test)\n",
    "    #active on everyn day   \n",
    "    #feature.summary('user_id','time_stamp','cnt','user_every_day_cnt',None,1,0)\n",
    "\n",
    "    # the sum of action from every user\n",
    "    feature.summary(['user_id'],None,'user_id_cnt',None,None,0,0)\n",
    "    print('over0')\n",
    "\n",
    "    #action in every month\n",
    "    feature.summary(['user_id'],'month','user_month_cnt','um',None,1,0)\n",
    "    feature.summary(['merchant_id'], 'month', 'merchant_month_cnt', 'mm',None, 1, 0)\n",
    "    print('over1')\n",
    "\n",
    "    # the sum of action got by every merchant\n",
    "    feature.summary(['merchant_id'],None,'merchant_id_cnt',None,None,0,0)\n",
    "    print('over2')\n",
    "\n",
    "    # every user active on every merchant\n",
    "    feature.summary(['user_id','merchant_id'],None,'user_id_merchant_id_cnt',None,None,0,0)\n",
    "    print('over3')\n",
    "\n",
    "    # the sum of actions got by every merchant in different gender or age\n",
    "    feature.summary(['merchant_id','gender'], None, 'merchant_gender_cnt', None, None, 0,0)\n",
    "    feature.summary(['merchant_id', 'age_range'], None, 'merchant_age_cnt', None, None, 0, 0)\n",
    "    print('over4')\n",
    "\n",
    "    # the unique of actions from every user on different item/cat/brand/merchant\n",
    "    feature.summary(['user_id'],'item_id','user_query_item_id_cnt',1,None,0,0)\n",
    "    feature.summary(['user_id'], 'cat_id', 'user_query_cat_id_cnt', 1, None, 0,0)\n",
    "    feature.summary(['user_id'], 'merchant_id', 'user_query_merchant_id_cnt', 1, None, 0,0)\n",
    "    feature.summary(['user_id'], 'brand_id', 'user_query_brand_id_cnt', 1, None, 0,0)\n",
    "    print('over5')\n",
    "\n",
    "    # consider of time\n",
    "    def timediff(t):\n",
    "        delta = datetime.datetime.strptime(str(max(t)), '%m%d') - datetime.datetime.strptime(str(min(t)), '%m%d')\n",
    "        return delta.days\n",
    "\n",
    "    feature.summary(['user_id'],'time_stamp','user_time_diff',None,timediff,0,0)\n",
    "    feature.summary(['merchant_id'], 'time_stamp', 'merchant_time_diff', None, timediff, 0, 0)\n",
    "    print('over6')\n",
    "\n",
    "    \n",
    "    feature.summary(['user_id', 'merchant_id'], 'time_stamp', 'user_merchant_time_diff', None, timediff, 0,0)\n",
    "    print('over7')\n",
    "    # how many months the user active on tne merchant\n",
    "    feature.summary(['user_id', 'merchant_id'],'month','user_merchant_month_cnt',1,None,0,0)\n",
    "    print('over8')\n",
    "    # how many days the user active on tne merchant\n",
    "    feature.summary(['user_id', 'merchant_id'],'time_stamp','user_merchant_day_cnt',1,None,0,0)\n",
    "\n",
    "    # every month the user active on the merchant\n",
    "    feature.summary(['user_id', 'merchant_id'],'month','cnt','month_act',None,1,0)\n",
    "    print('over9')\n",
    "    # the sum of active got by every merchant in different gender and  age\n",
    "    feature.summary(['merchant_id', 'gender', 'age_range'],None,'merchant_gender_age_cnt',None,None,0,0)\n",
    "    print('over10')\n",
    "    # the item/cat/brand/user of the merchant got max min mean \n",
    "    feature.summary(['merchant_id'], 'item_id', 'cnt', 'merchant_item_', [np.size, np.mean, np.max, np.min], 0,0)\n",
    "    feature.summary(['merchant_id'], 'cat_id', 'cnt', 'merchant_cat_', [np.size, np.mean, np.max, np.min], 0,0)\n",
    "    feature.summary(['merchant_id'], 'brand_id', 'cnt', 'merchant_brand_', [np.size, np.mean, np.max, np.min], 0,0)\n",
    "    feature.summary(['merchant_id'], 'user_id', 'cnt', 'merchant_user_', [np.size, np.mean, np.max, np.min], 0,0)\n",
    "    print('over11')\n",
    "    # every user active on every merchant's item\n",
    "    feature.summary(['user_id', 'merchant_id'],'item_id','cnt','user_merchant_item_',\n",
    "                     [np.size, np.mean, np.max, np.min],0,0)\n",
    "    # print('over12')\n",
    "    # every user active on every merchant's cat\n",
    "    feature.summary(['user_id', 'merchant_id'], 'cat_id', 'cnt', 'user_merchant_cat_',\n",
    "                 [np.size, np.mean, np.max, np.min], 0,0)\n",
    "    # print('over14')\n",
    "    # every user active on every merchant's brand\n",
    "    feature.summary(['user_id', 'merchant_id'], 'brand_id', 'cnt', 'user_merchant_brand_',\n",
    "                    [np.size, np.mean, np.max, np.min], 0,0)\n",
    "    # print('over14')\n",
    "    # different actions of  every user active on every merchant\n",
    "    feature.summary(['user_id', 'merchant_id'],'action_type','cnt','um_action',None,1,0)\n",
    "    # print('over15')\n",
    "    # the ratio of action\n",
    "    train = feature.train\n",
    "    test = feature.test\n",
    "    train['um_action_0_ratio'] = train['um_action_0'] / (train['um_action_0'] + train['um_action_1'] +\n",
    "                                                         train['um_action_2'] + train['um_action_3'] + 10)\n",
    "    \n",
    "    train['um_action_1_ratio'] = train['um_action_1'] / (train['um_action_0'] + train['um_action_1'] +\n",
    "                                                         train['um_action_2'] + train['um_action_3'] + 10)\n",
    "    \n",
    "    train['um_action_2_ratio'] = train['um_action_2'] / (train['um_action_0'] + train['um_action_1'] +\n",
    "                                                         train['um_action_2'] + train['um_action_3'] + 10)\n",
    "    \n",
    "    train['um_action_3_ratio'] = train['um_action_3'] / (train['um_action_0'] + train['um_action_1'] +\n",
    "                                                         train['um_action_2'] + train['um_action_3'] + 10)\n",
    "    \n",
    "    test['um_action_0_ratio'] = test['um_action_0'] / (test['um_action_0'] + test['um_action_1'] +\n",
    "                                                       test['um_action_2'] + test['um_action_3'] + 10)\n",
    "    \n",
    "    test['um_action_1_ratio'] = test['um_action_1'] / (test['um_action_0'] + test['um_action_1'] +\n",
    "                                                       test['um_action_2'] + test['um_action_3'] + 10)\n",
    "    \n",
    "    test['um_action_2_ratio'] = test['um_action_2'] / (test['um_action_0'] + test['um_action_1'] +\n",
    "                                                       test['um_action_2'] + test['um_action_3'] + 10)\n",
    "    \n",
    "    test['um_action_3_ratio'] = test['um_action_3'] / (test['um_action_0'] + test['um_action_1'] +\n",
    "                                                       test['um_action_2'] + test['um_action_3'] + 10)\n",
    "\n",
    "    feature.summary(['user_id'], 'item_id', 'cnt', 'user_item_', [np.size, np.mean, np.max, np.min], 0,0)\n",
    "    feature.summary(['user_id'], 'cat_id', 'cnt', 'user_cat_', [np.size, np.mean, np.max, np.min], 0,0)\n",
    "    feature.summary(['user_id'], 'brand_id', 'cnt', 'user_brand_',[np.size, np.mean, np.max, np.min], 0,0)\n",
    "    feature.summary(['user_id'], 'merchant_id', 'cnt', 'user_merchant_', [np.size, np.mean, np.max, np.min], 0,0)\n",
    "    print('over16')\n",
    "    # the ratio of every user's action on different merchant\n",
    "    train = feature.train\n",
    "    test = feature.test\n",
    "    train['um_u_ratio'] = train['user_id_merchant_id_cnt'] / train['user_id_cnt']\n",
    "    test['um_u_ratio'] = test['user_id_merchant_id_cnt'] / test['user_id_cnt']\n",
    "    train['um_m_ratio'] = train['user_id_merchant_id_cnt'] / train['merchant_id_cnt']\n",
    "    test['um_m_ratio'] = test['user_id_merchant_id_cnt'] / test['merchant_id_cnt']\n",
    "    print('over17')\n",
    "   \n",
    "\n",
    "    feature.summary(['merchant_id'],'action_type','cnt','merchant_action_cnt',None,1,0)\n",
    "    feature.summary(['user_id'], 'action_type', 'cnt', 'user_action_cnt', None, 1, 0)\n",
    "\n",
    "    print('over18')\n",
    "    # consder merchant_id, gender, age_range\n",
    "    feature.summary(['merchant_id', 'gender', 'age_range'],'item_id','cnt','merchant_gender_age_item_id_',\n",
    "                    [np.size, np.mean, np.max, np.min], 0,0)\n",
    "    feature.summary(['merchant_id', 'gender', 'age_range'], 'brand_id', 'cnt', 'merchant_gender_age_brand_id_',\n",
    "                    [np.size, np.mean, np.max, np.min], 0,0)\n",
    "    feature.summary(['merchant_id', 'gender', 'age_range'], 'cat_id', 'cnt', 'merchant_gender_age_cat_id_',\n",
    "                    [np.size, np.mean, np.max, np.min], 0,0)\n",
    "    print('over19')\n",
    "    \n",
    "    # every month how many unique user active on merchant\n",
    "    def count(x):\n",
    "        return len(set(x))\n",
    "   \n",
    "    feature.summary(['merchant_id','user_id'],'month','cnt','merchant_month_user_cnt',count,1,0)\n",
    "    train = feature.train\n",
    "    test = feature.test\n",
    "\n",
    "    # just consider of buy\n",
    "    feature = dataFeature(user_log[user_log['action_type']==2],train,test)\n",
    "\n",
    "    feature.summary(['merchant_id'], 'time_stamp', 'merchant_buy_time_diff', None, timediff, 0, 0)\n",
    "\n",
    "    feature.summary(['merchant_id'], 'month', 'merchant_month_cnt', 'merchant_month', None, 1, 0)\n",
    "    print('over20')\n",
    "    \n",
    "    # rebuy\n",
    "    def rebuy(x):\n",
    "        return len([i[0] for i in Counter(x).items() if i[1] > 1])\n",
    "    feature.summary(['merchant_id'],'user_id','merchant_allbuy_user_cnt',1,None,0,0)\n",
    "    feature.summary(['merchant_id'],'user_id','merchant_repeat_buy_user_cnt',None,rebuy,0,0)\n",
    "    # feature.summary(['merchant_id', 'cat_id'], 'user_id', 'merchant_cat_repeat_ratio', 'repeat_ratio',rebuy, 1,1)\n",
    "    print('over21')\n",
    "    \n",
    "    #how many people rebuy\n",
    "    feature.summary(['user_id'],'merchant_id','user_repeat_buy_cnt',None,rebuy,0,0)\n",
    "    train = feature.train\n",
    "    test = feature.test\n",
    "    train['user_repeat_buy_ratio'] = train['user_repeat_buy_cnt']/train['user_action_cnt_2']\n",
    "    test['user_repeat_buy_ratio'] = test['user_repeat_buy_cnt']/test['user_action_cnt_2']\n",
    "     \n",
    "    # if somebody rebuy , get the times \n",
    "    def rebuyuser(x):\n",
    "        return {i[0]: i[1] for i in Counter(x).items() if i[1] > 1}\n",
    "        \n",
    "    feature.summary(['merchant_id'],'user_id','repeat_user_list',None,rebuyuser,0,0)\n",
    "    print('over22')\n",
    "    def extra_user_repeat_cnt(x):\n",
    "        user_id = x['user_id']\n",
    "        repeat_user_list = x['repeat_user_list']\n",
    "        try:\n",
    "            return repeat_user_list[user_id]\n",
    "        except:\n",
    "            return 0\n",
    "    \n",
    "    train = feature.train\n",
    "    test = feature.test\n",
    "    train['usr_repeat_cnt'] = train[['user_id', 'repeat_user_list']].apply(extra_user_repeat_cnt, axis=1)\n",
    "    test['usr_repeat_cnt'] = test[['user_id', 'repeat_user_list']].apply(extra_user_repeat_cnt, axis=1)\n",
    "    train.to_csv('taobao/train.csv',index=False)\n",
    "    test.to_csv('taobao/test.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**it will take about 2 hours ~~~ it is a good idea to save the results**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part3: Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from gensim.models import Word2Vec\n",
    "def emb(data, f2):\n",
    "    emb_size =5\n",
    "    tmp = data.groupby(['user_id'], as_index=False)[f2].agg({'{}_list'.format(f2): list})\n",
    "    sentences = tmp['{}_list'.format(f2)].values.tolist()\n",
    "    del tmp['{}_list'.format(f2)]\n",
    "    for i in range(len(sentences)):\n",
    "        sentences[i] = [str(x) for x in sentences[i]]\n",
    "    model = Word2Vec(sentences, size=emb_size, window=2, min_count=1, sg=0, hs=1, seed=2019)\n",
    "    emb_matrix = []\n",
    "    for seq in tqdm(sentences):\n",
    "        vec = []\n",
    "        for w in seq:\n",
    "            if w in model:\n",
    "                vec.append(model[w])\n",
    "        if len(vec) > 0:\n",
    "            emb_matrix.append(np.mean(vec, axis=0))\n",
    "        else:\n",
    "            emb_matrix.append([0] * emb_size)\n",
    "    emb_matrix = np.array(emb_matrix)\n",
    "    for i in range(emb_size):\n",
    "        tmp['{}_emb_{}'.format(f2, i)] = emb_matrix[:, i]\n",
    "    del model, emb_matrix, sentences\n",
    "    return tmp\n",
    "\n",
    "\n",
    "for feat in ['item_id','cat_id','brand_id']:\n",
    "    t=emb(user_log,feat)\n",
    "    train=pd.merge(train,t, on=['user_id'], how='left')\n",
    "    predict=pd.merge(predict,t, on=['user_id'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**get the seq of click, deepwalk is also a good try**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part4:Under sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_sample_data_by_sample(df,percent=8):\n",
    "    most_data = df[df['label'] == 0]  \n",
    "    minority_data = df[df['label'] == 1]  \n",
    "    lower_data=most_data.sample(n=int(percent*len(minority_data)),replace=False,random_state=0,axis=0)\n",
    "    new_data=pd.concat([lower_data,minority_data])\n",
    "    return new_data \n",
    "train=lower_sample_data_by_sample(train)\n",
    "train=train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**over sample is also a good try**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part5:Train the model and merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from lightgbm import LGBMClassifier,Dataset\n",
    "from xgboost import XGBClassifier,DMatrix\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.fillna(-1)\n",
    "test = test.fillna(-1)\n",
    "train[['age_range', 'gender']] = train[['age_range', 'gender']].astype('int8')\n",
    "test[['age_range', 'gender']] = test[['age_range', 'gender']].astype('int8')\n",
    "label = train['label']\n",
    "trainNew = train.drop(['label'],axis=1)\n",
    "test.drop('prob',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = ['user_id','merchant_id','age_range', 'gender']\n",
    "def trainData(train_df,label_df):\n",
    "        skv = StratifiedKFold(n_splits=5, shuffle=True, random_state=2009)\n",
    "        trainX = []\n",
    "        trainY = []\n",
    "        testX = []\n",
    "        testY = []\n",
    "        for train_index, test_index in skv.split(X=train_df, y=label_df):\n",
    "            train_x, train_y, test_x, test_y = train_df.iloc[train_index, :], label_df.iloc[train_index], \\\n",
    "                                               train_df.iloc[test_index, :], label_df.iloc[test_index]\n",
    "\n",
    "            trainX.append(train_x)\n",
    "            trainY.append(train_y)\n",
    "            testX.append(test_x)\n",
    "            testY.append(test_y)\n",
    "        return trainX,trainY,testX,testY\n",
    "\n",
    "trainX,trainY,testX,testY = trainData(trainNew,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_xgbs = []\n",
    "pred_result = test[['merchant_id','user_id']]\n",
    "\n",
    "for i in range(5):\n",
    "        # xgb = XGBClassifier(n_estimators=2000, max_depth=5, learning_rate=0.02, \n",
    "        #                   eval_metric='auc', reg_lambda=1, random_state=10, n_jobs=8)\n",
    "        xgb = XGBClassifier(n_estimators=2000,learning_rate=0.008,eval_metric='auc',random_state=111)\n",
    "        xgb.fit(trainX[i],trainY[i],eval_set=[(testX[i],testY[i])],early_stopping_rounds=200,eval_metric='auc')\n",
    "        print(xgb.evals_result_)\n",
    "        pred = xgb.predict_proba(test, ntree_limit = xgb.best_iteration)[:,1]\n",
    "        pred_xgbs.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_cats = []\n",
    "for i in range(5):\n",
    "        # cat = CatBoostClassifier(learning_rate=0.02, iterations=5000, eval_metric='AUC', od_wait=50,\n",
    "        #                          od_type='Iter', random_state=10, thread_count=8, l2_leaf_reg=1)\n",
    "        cat = CatBoostClassifier(learning_rate=0.008, iterations=2000, eval_metric='AUC', random_state=198)\n",
    "        cat.fit(trainX[i], trainY[i], eval_set=[(testX[i], testY[i])], early_stopping_rounds=100,\n",
    "                use_best_model=True,cat_features=cat_features)\n",
    "        print(cat.evals_result_)\n",
    "        pred = cat.predict_proba(test, ntree_end=cat.best_iteration_)[:, 1]\n",
    "        pred_cats.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_lgbms= []\n",
    "for i in range(5):\n",
    "        # lgbm = LGBMClassifier(n_estimators=2000,objective='binary',num_leaves=31,max_depth=5,learning_rate=0.02,\n",
    "        #                       reg_lambda=1,metric=['auc'], random_state=10,n_jobs=-1)\n",
    "        lgbm = LGBMClassifier(n_estimators=1000,learning_rate=0.02,metric=['auc'], random_state=20)\n",
    "        lgbm.fit(trainX[i],trainY[i],eval_set=[(testX[i],testY[i])],early_stopping_rounds=100,eval_metric='auc',\n",
    "                 categorical_feature=cat_features)\n",
    "        print(lgbm.evals_result_)\n",
    "        pred = lgbm.predict_proba(test,num_iteration=lgbm.best_iteration_)[:,1]\n",
    "        pred_lgbms.append(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**as you know lgb cost the least time and get the greatest score, the combination of this three models always promote it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_ver(x):\n",
    "        return np.log(x/(1-x))\n",
    "def sigmoid(x):\n",
    "        return 1/(1 + np.e**(-x))\n",
    "\n",
    "pred_t = np.zeros(len(predict))\n",
    "    \n",
    "for i in range(5):\n",
    "        pred_t += (sigmoid_ver(pred_lgbms[i])  + sigmoid_ver(pred_xgbs[i])+ sigmoid_ver(pred_cats[i])\n",
    "\n",
    "        \n",
    "result = sigmoid(pred_t/15)\n",
    "pred_result['prob'] = result\n",
    "pred_result[['user_id','merchant_id','prob']].to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part6:Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "import numpy as np\n",
    "\n",
    "clf1 = KNeighborsClassifier(n_neighbors=1)\n",
    "clf2 = RandomForestClassifier(random_state=1)\n",
    "clf3 = GaussianNB()\n",
    "lr = LogisticRegression()\n",
    "sclf = StackingClassifier(classifiers=[clf1, clf2, clf3], meta_classifier=lr)\n",
    "\n",
    "print('3-fold cross validation:\\n')\n",
    "\n",
    "for clf, label in zip(\n",
    "    [clf1, clf2, clf3, sclf],\n",
    "    ['KNN', 'Random Forest', 'Naive Bayes', 'StackingClassifier']):\n",
    "\n",
    "    scores = model_selection.cross_val_score(clf, train_x , train_y, cv=3, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
